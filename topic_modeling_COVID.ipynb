{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy-langdetect\n",
    "#!pip install language-detector\n",
    "#!pip install symspellpy\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import time\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# import seaborn as sns\n",
    "# from mpl_toolkits.mplot3d import Axes3D"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Read dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341713, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhipe\\Anaconda3\\envs\\tm\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "meta = pd.read_csv(\"../metadata.csv\")\n",
    "print(meta.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   cord_uid                                       sha source_x  \\\n0  ug7v899j  d1aafb70c066a2068b02786f8929fd9c900897fb      PMC   \n1  02tnwd4m  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d      PMC   \n2  ejv2xln0  06ced00a5fc04215949aa72528f2eeaae1d58927      PMC   \n3  2b73a28n  348055649b6b8cf2b9a376498df9bf41f7123605      PMC   \n4  9785vg6d  5f48792a5fa08bed9f56016f4981ae2ca6031b32      PMC   \n\n                                               title                    doi  \\\n0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n3               Role of endothelin-1 in lung disease           10.1186/rr44   \n4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n\n      pmcid pubmed_id license  \\\n0  PMC35282  11472636   no-cc   \n1  PMC59543  11667967   no-cc   \n2  PMC59549  11667972   no-cc   \n3  PMC59574  11686871   no-cc   \n4  PMC59580  11686888   no-cc   \n\n                                            abstract publish_time  \\\n0  OBJECTIVE: This retrospective chart review des...   2001-07-04   \n1  Inflammatory diseases of the respiratory tract...   2000-08-15   \n2  Surfactant protein-D (SP-D) participates in th...   2000-08-25   \n3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   2001-02-22   \n4  Respiratory syncytial virus (RSV) and pneumoni...   2001-05-11   \n\n                                             authors         journal  mag_id  \\\n0                Madani, Tariq A; Al-Ghamdi, Aisha A  BMC Infect Dis     NaN   \n1  Vliet, Albert van der; Eiserich, Jason P; Cros...      Respir Res     NaN   \n2                                    Crouch, Erika C      Respir Res     NaN   \n3  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M      Respir Res     NaN   \n4  Domachowske, Joseph B; Bonville, Cynthia A; Ro...      Respir Res     NaN   \n\n  who_covidence_id arxiv_id  \\\n0              NaN      NaN   \n1              NaN      NaN   \n2              NaN      NaN   \n3              NaN      NaN   \n4              NaN      NaN   \n\n                                      pdf_json_files  \\\n0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n1  document_parses/pdf_json/6b0567729c2143a66d737...   \n2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n\n                               pmc_json_files  \\\n0  document_parses/pmc_json/PMC35282.xml.json   \n1  document_parses/pmc_json/PMC59543.xml.json   \n2  document_parses/pmc_json/PMC59549.xml.json   \n3  document_parses/pmc_json/PMC59574.xml.json   \n4  document_parses/pmc_json/PMC59580.xml.json   \n\n                                                 url  s2_id  \n0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...    NaN  \n1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  \n4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...    NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cord_uid</th>\n      <th>sha</th>\n      <th>source_x</th>\n      <th>title</th>\n      <th>doi</th>\n      <th>pmcid</th>\n      <th>pubmed_id</th>\n      <th>license</th>\n      <th>abstract</th>\n      <th>publish_time</th>\n      <th>authors</th>\n      <th>journal</th>\n      <th>mag_id</th>\n      <th>who_covidence_id</th>\n      <th>arxiv_id</th>\n      <th>pdf_json_files</th>\n      <th>pmc_json_files</th>\n      <th>url</th>\n      <th>s2_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ug7v899j</td>\n      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n      <td>PMC</td>\n      <td>Clinical features of culture-proven Mycoplasma...</td>\n      <td>10.1186/1471-2334-1-6</td>\n      <td>PMC35282</td>\n      <td>11472636</td>\n      <td>no-cc</td>\n      <td>OBJECTIVE: This retrospective chart review des...</td>\n      <td>2001-07-04</td>\n      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n      <td>BMC Infect Dis</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>02tnwd4m</td>\n      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n      <td>PMC</td>\n      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n      <td>10.1186/rr14</td>\n      <td>PMC59543</td>\n      <td>11667967</td>\n      <td>no-cc</td>\n      <td>Inflammatory diseases of the respiratory tract...</td>\n      <td>2000-08-15</td>\n      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n      <td>Respir Res</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ejv2xln0</td>\n      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n      <td>PMC</td>\n      <td>Surfactant protein-D and pulmonary host defense</td>\n      <td>10.1186/rr19</td>\n      <td>PMC59549</td>\n      <td>11667972</td>\n      <td>no-cc</td>\n      <td>Surfactant protein-D (SP-D) participates in th...</td>\n      <td>2000-08-25</td>\n      <td>Crouch, Erika C</td>\n      <td>Respir Res</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2b73a28n</td>\n      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n      <td>PMC</td>\n      <td>Role of endothelin-1 in lung disease</td>\n      <td>10.1186/rr44</td>\n      <td>PMC59574</td>\n      <td>11686871</td>\n      <td>no-cc</td>\n      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n      <td>2001-02-22</td>\n      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n      <td>Respir Res</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9785vg6d</td>\n      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n      <td>PMC</td>\n      <td>Gene expression in epithelial cells in respons...</td>\n      <td>10.1186/rr61</td>\n      <td>PMC59580</td>\n      <td>11686888</td>\n      <td>no-cc</td>\n      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n      <td>2001-05-11</td>\n      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n      <td>Respir Res</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter meta file by selecting only papers after 2020."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247255  papers are available after 2020 Jan 1.\n"
     ]
    }
   ],
   "source": [
    "meta[\"publish_time\"] = pd.to_datetime(meta[\"publish_time\"])\n",
    "meta[\"publish_year\"] = (pd.DatetimeIndex(meta['publish_time']).year)\n",
    "meta[\"publish_month\"] = (pd.DatetimeIndex(meta['publish_time']).month)\n",
    "meta = meta[meta[\"publish_year\"] == 2020]\n",
    "print(meta.shape[0], \" papers are available after 2020 Jan 1.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "publish_month  publish_year\n1.0            2020.0          123040\n2.0            2020.0            1881\n3.0            2020.0            4861\n4.0            2020.0           12299\n5.0            2020.0           18171\n6.0            2020.0           17611\n7.0            2020.0           17723\n8.0            2020.0           16545\n9.0            2020.0           17559\n10.0           2020.0           14888\n11.0           2020.0            2280\n12.0           2020.0             397\ndtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meta.groupby([\"publish_month\",\"publish_year\"]).size().plot(kind='bar')\n",
    "meta.groupby([\"publish_month\",\"publish_year\"]).size()\n",
    "#tmp = meta.groupby([\"journal\"])[\"journal\"].count()\n",
    "#tmp=tmp.sort_index()\n",
    "#print(tmp.size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id',\n",
      "       'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
      "       'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files',\n",
      "       'url', 's2_id', 'publish_year', 'publish_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(meta.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Count how many documents has abstract"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163820  papers have abstract available.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "index = []\n",
    "for i in range(len(meta)):\n",
    "    #print(i)\n",
    "    if type(meta.iloc[i, 8])== float:\n",
    "        count += 1\n",
    "    else:\n",
    "        index.append(i)\n",
    "\n",
    "print(len(index), \" papers have abstract available.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataframe with all abstracts and use it as input corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            abstract  index\n0  BACKGROUND: Dexmedetomidine has been reported ...      0\n1  BACKGROUND: Global end-diastolic volume (GEDV)...      1\n2  BACKGROUND: Human metapneumovirus (HMPV) is an...      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abstract</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BACKGROUND: Dexmedetomidine has been reported ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BACKGROUND: Global end-diastolic volume (GEDV)...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BACKGROUND: Human metapneumovirus (HMPV) is an...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = meta.iloc[index, 8]\n",
    "documents=documents.reset_index()\n",
    "documents.head()\n",
    "documents.drop(\"index\", inplace = True, axis = 1)\n",
    "documents[\"index\"] = documents.index.values\n",
    "documents.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Preprocessing functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82781\n"
     ]
    }
   ],
   "source": [
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "print(sym_spell.word_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentence level preprocess\n",
    "* Lowercase + base filter\n",
    "* Some basic normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import re\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "    return s.strip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from language_detector import detect_language\n",
    "# language detection\n",
    "def f_lan(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: boolean (s is English)\n",
    "    \"\"\"\n",
    "    # some reviews are actually english but biased toward french\n",
    "    return detect_language(s) in {'English', 'French','Spanish','Chinese'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Word level preprocess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "# filtering out punctuations and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with punct and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "# selecting nouns\n",
    "def f_noun(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "# typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        # print(suggestions[0])\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            w_list_fixed.append(word)\n",
    "    return w_list_fixed\n",
    "\n",
    "# stemming if doing word-wise\n",
    "p_stemmer = PorterStemmer()\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with stemming\n",
    "    \"\"\"\n",
    "    return [p_stemmer.stem(word) for word in w_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# filtering out stop words\n",
    "# create English stop words list\n",
    "from stop_words import get_stop_words\n",
    "stop_words = (list(\n",
    "    set(get_stop_words('en'))\n",
    "    |set(get_stop_words('es'))\n",
    "    |set(get_stop_words('de'))\n",
    "    |set(get_stop_words('it'))\n",
    "    |set(get_stop_words('ca'))\n",
    "    |set(get_stop_words('pt'))\n",
    "    |set(get_stop_words('pl'))\n",
    "    |set(get_stop_words('da'))\n",
    "    |set(get_stop_words('ru'))\n",
    "    |set(get_stop_words('sv'))\n",
    "    |set(get_stop_words('sk'))\n",
    "    |set(get_stop_words('nl'))\n",
    "))\n",
    "\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    filtering out stop words\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in stop_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw review texts\n",
    "    :param rw: review to be processed\n",
    "    :return: sentence level pre-processed review\n",
    "    \"\"\"\n",
    "    s = f_base(rw)\n",
    "    if not f_lan(s):\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = f_punct(w_list)\n",
    "    w_list = f_noun(w_list)\n",
    "    w_list = f_typo(w_list)\n",
    "    w_list = f_stem(w_list)\n",
    "    w_list = f_stopw(w_list)\n",
    "\n",
    "    return w_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preprocessing raw text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "documents = documents.fillna('')  # only the comments has NaN's\n",
    "docs = documents.abstract"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "samp_size = 5100 #51000\n",
    "ntopic = 10\n",
    "np.random.seed(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "n_docs = len(docs)\n",
    "sentences = []  # sentence level preprocessed\n",
    "token_lists = []  # word level preprocessed\n",
    "idx_in = []  # index of sample selected\n",
    "samp = np.random.choice(n_docs, samp_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0    BACKGROUND: Dexmedetomidine has been reported ...\n1    BACKGROUND: Global end-diastolic volume (GEDV)...\n2    BACKGROUND: Human metapneumovirus (HMPV) is an...\n3    BACKGROUND: Antimicrobial resistance (AMR) com...\n4    Glycoconjugate vaccines based on bacterial cap...\nName: abstract, dtype: object"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %\r"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(samp):\n",
    "    sentence = preprocess_sent(docs[idx])\n",
    "    token_list = preprocess_word(sentence)\n",
    "    if token_list:\n",
    "        idx_in.append(idx)\n",
    "        sentences.append(sentence)\n",
    "        token_lists.append(token_list)\n",
    "    print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5062\n",
      "5062\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(token_lists))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Define model object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class Topic_Model:\n",
    "    def __init__(self, k=10, method='TFIDF'):\n",
    "        \"\"\"\n",
    "        :param k: number of topics\n",
    "        :param method: method chosen for the topic model\n",
    "        \"\"\"\n",
    "        if method not in {'TFIDF', 'LDA'}:\n",
    "            raise Exception('Invalid method!')\n",
    "        self.k = k\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.cluster_model = None\n",
    "        self.ldamodel = None\n",
    "        self.vec = {}\n",
    "        self.gamma = 15  # parameter for relative importance of lda\n",
    "        self.method = method\n",
    "        self.AE = None\n",
    "        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    def vectorize(self, sentences, token_lists, method=None):\n",
    "        \"\"\"\n",
    "        Get vecotr representations from selected methods\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        self.dictionary = corpora.Dictionary(token_lists)\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'TFIDF':\n",
    "            print('Getting vector representations for TF-IDF ...')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            vec = tfidf.fit_transform(sentences)\n",
    "            print('Getting vector representations for TF-IDF. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'LDA':\n",
    "            print('Getting vector representations for LDA ...')\n",
    "            if not self.ldamodel:\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "\n",
    "            def get_vec_lda(model, corpus, k):\n",
    "                \"\"\"\n",
    "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "                \"\"\"\n",
    "                n_doc = len(corpus)\n",
    "                vec_lda = np.zeros((n_doc, k))\n",
    "                for i in range(n_doc):\n",
    "                    # get the distribution for the i-th document in corpus\n",
    "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
    "                        vec_lda[i, topic] = prob\n",
    "\n",
    "                return vec_lda\n",
    "\n",
    "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
    "            print('Getting vector representations for LDA. Done!')\n",
    "            return vec\n",
    "\n",
    "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
    "        \"\"\"\n",
    "        Fit the topic model for selected method given the preprocessed data\n",
    "        :docs: list of documents, each doc is preprocessed as tokens\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        # Default clustering method\n",
    "        if m_clustering is None:\n",
    "            m_clustering = KMeans\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        if not self.dictionary:\n",
    "            self.dictionary = corpora.Dictionary(token_lists)\n",
    "            # convert tokenized documents into a document-term matrix\n",
    "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        ####################################################\n",
    "        #### Getting ldamodel or vector representations ####\n",
    "        ####################################################\n",
    "\n",
    "        if method == 'LDA':\n",
    "            if not self.ldamodel:\n",
    "                print('Fitting LDA ...')\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "                print('Fitting LDA Done!')\n",
    "        else:\n",
    "            print('Clustering embeddings ...')\n",
    "            self.cluster_model = m_clustering(self.k)\n",
    "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
    "            self.cluster_model.fit(self.vec[method])\n",
    "            print('Clustering embeddings. Done!')\n",
    "\n",
    "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
    "        \"\"\"\n",
    "        Predict topics for new_documents\n",
    "        \"\"\"\n",
    "        # Default as False\n",
    "        out_of_sample = out_of_sample is not None\n",
    "\n",
    "        if out_of_sample:\n",
    "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "            if self.method != 'LDA':\n",
    "                vec = self.vectorize(sentences, token_lists)\n",
    "                print(vec)\n",
    "        else:\n",
    "            corpus = self.corpus\n",
    "            vec = self.vec.get(self.method, None)\n",
    "\n",
    "        if self.method == \"LDA\":\n",
    "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
    "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
    "                                    corpus)))\n",
    "        else:\n",
    "            lbs = self.cluster_model.predict(vec)\n",
    "        return lbs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA ...\n",
      "Fitting LDA Done!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "method = \"LDA\" #\"LDA_BERT\"\n",
    "# Define the topic model object\n",
    "tm = Topic_Model(k = ntopic, method = method)\n",
    "# Fit the topic model by chosen method\n",
    "tm.fit(sentences, token_lists)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Evaluate using metrics\n",
    "with open(\"{}.file\".format(tm.id), \"wb\") as f:\n",
    "    pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: 0.4644990408565503\n",
      "Silhouette Score: None\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "# import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4644990408565503\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(model=tm.ldamodel, texts=token_lists, corpus=tm.corpus,\n",
    "                    dictionary=tm.dictionary, coherence=\"c_v\")\n",
    "print(cm.get_coherence())\n",
    "print('Coherence:', cm.get_coherence())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.033*\"studi\" + 0.019*\"result\" + 0.016*\"particip\" + 0.015*\"survey\" + 0.015*\"student\" + 0.013*\"anxieti\" + 0.013*\"health\" + 0.011*\"level\" + 0.011*\"effect\" + 0.010*\"method\"')\n",
      "(1, '0.037*\"test\" + 0.030*\"method\" + 0.026*\"sampl\" + 0.025*\"detect\" + 0.022*\"result\" + 0.018*\"sensit\" + 0.015*\"assay\" + 0.013*\"perform\" + 0.012*\"valu\" + 0.011*\"system\"')\n",
      "(2, '0.025*\"infect\" + 0.024*\"drug\" + 0.023*\"viru\" + 0.022*\"vaccin\" + 0.020*\"coronaviru\" + 0.018*\"protein\" + 0.017*\"diseas\" + 0.013*\"respiratori\" + 0.011*\"cell\" + 0.010*\"studi\"')\n",
      "(3, '0.094*\"patient\" + 0.024*\"diseas\" + 0.018*\"studi\" + 0.017*\"treatment\" + 0.016*\"infect\" + 0.016*\"case\" + 0.015*\"risk\" + 0.013*\"result\" + 0.013*\"hospit\" + 0.013*\"group\"')\n",
      "(4, '0.057*\"cancer\" + 0.011*\"tutor\" + 0.009*\"water\" + 0.009*\"energi\" + 0.008*\"studi\" + 0.007*\"result\" + 0.007*\"analysi\" + 0.007*\"seed\" + 0.006*\"eye\" + 0.006*\"effect\"')\n",
      "(5, '0.046*\"cell\" + 0.015*\"express\" + 0.013*\"activ\" + 0.013*\"effect\" + 0.013*\"group\" + 0.012*\"cytokin\" + 0.012*\"level\" + 0.011*\"lung\" + 0.010*\"respons\" + 0.009*\"tissu\"')\n",
      "(6, '0.030*\"pm\" + 0.027*\"imag\" + 0.021*\"method\" + 0.015*\"ga\" + 0.012*\"featur\" + 0.011*\"network\" + 0.011*\"tree\" + 0.010*\"formula\" + 0.009*\"date\" + 0.009*\"text\"')\n",
      "(7, '0.038*\"women\" + 0.025*\"health\" + 0.023*\"popul\" + 0.022*\"age\" + 0.016*\"risk\" + 0.012*\"year\" + 0.012*\"pregnanc\" + 0.012*\"preval\" + 0.011*\"state\" + 0.009*\"peopl\"')\n",
      "(8, '0.030*\"health\" + 0.023*\"care\" + 0.017*\"pandem\" + 0.013*\"research\" + 0.012*\"system\" + 0.011*\"servic\" + 0.010*\"articl\" + 0.010*\"practic\" + 0.009*\"manag\" + 0.008*\"commun\"')\n",
      "(9, '0.032*\"case\" + 0.031*\"model\" + 0.023*\"data\" + 0.018*\"number\" + 0.016*\"transmiss\" + 0.016*\"countri\" + 0.015*\"rate\" + 0.013*\"diseas\" + 0.013*\"infect\" + 0.012*\"measur\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in tm.ldamodel.print_topics():\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Visualization\n",
    "\n",
    "Visualization functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_proj(embedding, lbs):\n",
    "    \"\"\"\n",
    "    Plot UMAP embeddings\n",
    "    :param embedding: UMAP (or other) embeddings\n",
    "    :param lbs: labels\n",
    "    \"\"\"\n",
    "    n = len(embedding)\n",
    "    counter = Counter(lbs)\n",
    "    for i in range(len(np.unique(lbs))):\n",
    "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
    "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n",
    "\n",
    "def visualize(model):\n",
    "    \"\"\"\n",
    "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
    "    :param model: Topic_Model object\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    reducer = umap.UMAP()\n",
    "    print('Calculating UMAP projection ...')\n",
    "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
    "    print('Calculating UMAP projection. Done!')\n",
    "    plot_proj(vec_umap, model.cluster_model.labels_)\n",
    "    dr = '/kaggle/working/contextual_topic_identification/docs/images/{}/{}'.format(model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig('/kaggle/working/2D_vis')\n",
    "\n",
    "def get_wordcloud(model, token_lists, topic):\n",
    "    \"\"\"\n",
    "    Get word cloud of each topic from fitted model\n",
    "    :param model: Topic_Model object\n",
    "    :param sentences: preprocessed sentences from docs\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    print('Getting wordcloud for topic {} ...'.format(topic))\n",
    "    lbs = model.cluster_model.labels_\n",
    "    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=560,\n",
    "                          background_color='white', collocations=False,\n",
    "                          min_font_size=10).generate(tokens)\n",
    "\n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize=(8, 5.6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    dr = '/kaggle/working/{}/{}'.format(model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig('/kaggle/working' + '/Topic' + str(topic) + '_wordcloud')\n",
    "    print('Getting wordcloud for topic {}. Done!'.format(topic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence: 0.4644990408565503\n",
      "Silhouette Score: None\n"
     ]
    }
   ],
   "source": [
    "# visualize and save img\n",
    "visualize(tm)\n",
    "for i in range(tm.k):\n",
    "    get_wordcloud(tm, token_lists, i)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3d929fb3",
   "language": "python",
   "display_name": "PyCharm (textmining)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}